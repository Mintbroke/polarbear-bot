# Use Python provider; it auto-detects requirements.txt
providers = ["python"]

[variables]
PYTHONUNBUFFERED = "1"
# Keep Ollama private inside the container
OLLAMA_HOST = "127.0.0.1:11434"
# You can override this in Railway â†’ Variables
OLLAMA_START_MODEL = "llama3.2:1b"

[phases.setup]
# System packages you previously apt-installed
nixPkgs = [
  "ffmpeg",
  "opus",         # libopus
  "libsodium",
  "postgresql",   # libpq / psql
  "pkg-config",
  "curl",
  "bash",
  "git"
]

[phases.install]
# Optional: install Ollama binary at build time (idempotent)
cmds = [
  "if ! command -v ollama >/dev/null; then curl -fsSL https://ollama.com/install.sh | sh; fi"
]

[start]
# Use a script so we can boot Ollama, wait, ensure model, then run the bot
cmd = "bash ./start.sh"
